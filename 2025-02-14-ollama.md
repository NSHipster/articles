---
title: Ollama
author: Mattt
category: Miscellaneous
excerpt: >-
  While we wait for Apple Intelligence to arrive on our devices,
  something remarkable is already running on our Macs.
  Think of it as a locavore approach to artificial intelligence:
  homegrown, sustainable, and available year-round.
status:
  swift: 6.0
---

> "Only Apple can do this" 
> <cite>Variously attributed to Tim Cook</cite>

Apple introduced [Apple Intelligence](https://www.apple.com/apple-intelligence/) at WWDC 2024.
After waiting almost a year for Apple to, 
in Craig Federighi's words, _"get it right"_, 
its promise of "AI for the rest of us" feels just as distant as ever.

<aside class="parenthetical">
Can we take a moment to appreciate the name?
**A**pple **I**ntelligence. 
AI.
That's some S-tier semantic appropriation.
On the level of jumping on "podcast" before anyone knew what else to call that.
</aside>

While we wait for Apple Intelligence to arrive on our devices,
something remarkable is already running on our Macs.
Think of it as a locavore approach to artificial intelligence:
homegrown, sustainable, and available year-round.

This week on NSHipster,
we'll look at how you can use Ollama to run 
<abbr title="large language models">LLMs</abbr> locally on your Mac â€”
both as an end-user and as a developer.

---

## What is Ollama?

Ollama is the easiest way to run large language models on your Mac. 
You can think of it as "Docker for LLMs" - 
a way to pull, run, and manage AI models as easily as containers.

Download Ollama with [Homebrew](https://brew.sh/) 
or directly from [their website](https://ollama.com/download).
Then pull and run [llama3.2](https://ollama.com/library/llama3.2) (2GB).

```terminal
$ brew install --cask ollama
$ ollama run llama3.2
>>> Tell me a joke about Swift programming.
What's a Apple developer's favorite drink? 
The Kool-Aid.
```

Under the hood, 
Ollama is powered by [llama.cpp](https://github.com/ggerganov/llama.cpp).
But where llama.cpp provides the engine,
Ollama gives you a vehicle you'd actually want to drive â€”
handling all the complexity of model management, optimization, and inference.

Similar to how Dockerfiles define container images, 
Ollama uses <dfn>Modelfiles</dfn> to configure model behavior:

```dockerfile
FROM mistral:latest
PARAMETER temperature 0.7
TEMPLATE """
{{- if .First }}
You are a helpful assistant.
{{- end }}

User: {{.Prompt}}
Assistant: """
```

Ollama uses the [Open Container Initiative (OCI)](https://opencontainers.org)
standard to distribute models. 
Each model is split into layers and described by a manifest,
the same approach used by Docker containers:

```json
{
  "mediaType": "application/vnd.oci.image.manifest.v1+json",
  "config": {
    "mediaType": "application/vnd.ollama.image.config.v1+json",
    "digest": "sha256:..."
  },
  "layers": [
    {
      "mediaType": "application/vnd.ollama.image.layer.v1+json",
      "digest": "sha256:...",
      "size": 4019248935
    }
  ]
}
```

Overall, Ollama's approach is thoughtful and well-engineered.
And best of all, _it just works_.

## What's the big deal about running models locally?

[Jevons paradox](https://en.wikipedia.org/wiki/Jevons_paradox) states that, 
as something becomes more efficient, we tend to use _more_ of it, not less.

Having AI on your own device changes everything.
When computation becomes essentially free,
you start to see intelligence differently.

While frontier models like GPT-4 and Claude are undeniably miraculous, 
there's something to be said for the small miracle of running open models locally.

- **Privacy**: 
  Your data never leaves your device. 
  Essential for working with sensitive information.
- **Cost**: 
  Run 24/7 without usage meters ticking. 
  No more rationing prompts like '90s cell phone minutes.
  Just a fixed, up-front cost for unlimited inference.
- **Latency**: 
  No network round-trips means faster responses. 
  Your `/M\d Mac((Book( Pro| Air)?)|Mini|Studio)/` can easily generate dozens of tokens per second. 
  (Try to keep up!)
- **Control**: 
  No black-box <a href="https://knowyourmeme.com/photos/2546581-shoggoth-with-smiley-face-artificial-intelligence"><abbr title="Reinforcement Learning from Human Feedback">RLHF</abbr></a> or censorship. 
  The AI works for you, not the other way around.
- **Reliability**: 
  No outages or API quota limits.
  100% uptime for your [exocortex](https://en.wiktionary.org/wiki/exocortex).
  Like having Wikipedia on a thumb drive.

## Building macOS Apps with Ollama

Ollama also exposes an [HTTP API](https://github.com/ollama/ollama/blob/main/docs/api.md) on port 11434
([leetspeak](https://en.wikipedia.org/wiki/Leet) for llama ðŸ¦™).
This makes it easy to integrate with any programming language or tool.

To that end, we've created the [Ollama Swift package](https://github.com/mattt/ollama-swift)
to help developers integrate Ollama into their apps.

### Text Completions

The simplest way to use a language model is to generate text from a prompt:

```swift
import Ollama

let client = Client.default
let response = try await client.generate(
    model: "llama3.2",
    prompt: "Tell me a joke about Swift programming.",
    options: ["temperature": 0.7]
)
print(response.response)
// How many Apple engineers does it take to document an API? 
// None - that's what WWDC videos are for.
```

### Chat Completions

For more structured interactions, 
you can use the chat API to maintain a conversation with multiple messages and different roles:

```swift
let initialResponse = try await client.chat(
    model: "llama3.2",
    messages: [
        .system("You are a helpful assistant."),
        .user("What city is Apple located in?")
    ]
)
print(initialResponse.message.content)
// Apple's headquarters, known as the Apple Park campus, is located in Cupertino, California.
// The company was originally founded in Los Altos, California, and later moved to Cupertino in 1997.

let followUp = try await client.chat(
    model: "llama3.2",
    messages: [
        .system("You are a helpful assistant."),
        .user("What city is Apple located in?"),
        .assistant(initialResponse.message.content),
        .user("Please summarize in a single word")
    ]
)
print(followUp.message.content)
// Cupertino
```

### Generating text embeddings

[Embeddings](https://en.wikipedia.org/wiki/Word_embedding) 
convert text into high-dimensional vectors that capture semantic meaning. 
These vectors can be used to find similar content or perform semantic search.

For example, if you wanted to find documents similar to a user's query:

```swift
let documents: [String] = <#...#>

// Convert text into vectors we can compare for similarity
let embeddings = try await client.embeddings(
    model: "nomic-embed-text", 
    texts: documents
)

/// Finds relevant documents
func findRelevantDocuments(
    for query: String, 
    threshold: Float = 0.7, // cutoff for matching, tunable
    limit: Int = 5
) async throws -> [String] {
    // Get embedding for the query
    let [queryEmbedding] = try await client.embeddings(
        model: "llama3.2",
        texts: [query]
    )
 
    // See: https://en.wikipedia.org/wiki/Cosine_similarity
    func cosineSimilarity(_ a: [Float], _ b: [Float]) -> Float {
        let dotProduct = zip(a, b).map(*).reduce(0, +)
        let magnitude = { sqrt($0.map { $0 * $0 }.reduce(0, +)) }
        return dotProduct / (magnitude(a) * magnitude(b))
    }
    
    // Find documents above similarity threshold
    let rankedDocuments = zip(embeddings, documents)
        .map { embedding, document in
            (similarity: cosineSimilarity(embedding, queryEmbedding),
             document: document)
        }
        .filter { $0.similarity >= threshold }
        .sorted { $0.similarity > $1.similarity }
        .prefix(limit)
    
    return rankedDocuments.map(\.document)
}
```

{% info %}
For simple use cases, 
you can also use Apple's 
[Natural Language framework](https://developer.apple.com/documentation/naturallanguage/)
for text embeddings.
They're fast and don't require additional dependencies.

```swift
import NaturalLanguage

let embedding = NLEmbedding.wordEmbedding(for: .english)
let vector = embedding?.vector(for: "swift")
```
{% endinfo %}

### Building a RAG System

Embeddings really shine when combined with text generation in a 
<abbr title="Retrieval Augmented Generation">RAG</abbr> (Retrieval Augmented Generation) workflow. 
Instead of asking the model to generate information from its training data, 
we can ground its responses in our own documents by:

1. Converting documents into embeddings 
2. Finding relevant documents based on the query
3. Using those documents as context for generation

Here's a simple example:

```swift
let query = "What were AAPL's earnings in Q3 2024?"
let relevantDocs = try await findRelevantDocuments(query: query)
let context = """
    Use the following documents to answer the question. 
    If the answer isn't contained in the documents, say so.
    
    Documents:
    \(relevantDocs.joined(separator: "\n---\n"))
    
    Question: \(query)
    """

let response = try await client.generate(
    model: "llama3.2",
    prompt: context
)
```

---

To summarize:
Different models have different capabilities.

* Models like [llama3.2](https://ollama.com/library/llama3.2) 
  and [deepseek-r1](https://ollama.com/library/deepseek-r1) 
  generate text.
  * Some text models have "base" or "instruct" variants,
    suitable for fine-tuning or chat completion, respectively.
  * Some text models are tuned to support [tool use](https://ollama.com/blog/tool-support),
    which let them perform more complex tasks and interact with the outside world.

* Models like [llama3.2-vision](https://ollama.com/library/llama3.2-vision)
  can take images along with text as inputs.

* Models like [nomic-embed-text](https://ollama.com/library/nomic-embed-text)
  create numerical vectors that capture semantic meaning.

With Ollama, 
you get unlimited access to a wealth of these and many more open-source language models.

---

So, what can you build with all of this?  
Here's just one example:

### Nominate.app

[Nominate](https://github.com/nshipster/nominate) 
is a macOS app that uses Ollama to intelligently rename PDF files based on their contents.

Like many of us striving for a paperless lifestyle,
you might find yourself scanning documents only to end up with 
cryptically-named PDFs like `Scan2025-02-03_123456.pdf`.
Nominate solves this by combining AI with traditional NLP techniques
to automatically generate descriptive filenames based on document contents.

<video width="831" autoplay loop muted playsinline>
    <source src="{% asset ollama-nominate-screen-recording.mov @path %}" type="video/mp4"/>
</video>

<br/>

The app leverages several technologies we've discussed:
- Ollama's API for content analysis via the `ollama-swift` package
- Apple's PDFKit for OCR
- The Natural Language framework for text processing
- Foundation's `DateFormatter` for parsing dates

{% info %}
Nominate performs all processing locally. Your documents never leave your computer. This is a key advantage of running models locally versus using cloud APIs.
{% endinfo %}

## Looking Ahead

> "The future is already here â€“ it's just not evenly distributed yet."
> <cite>William Gibson</cite>

Think about the timelines:
- Apple Intelligence was announced last year.  
- Swift came out 10 years ago.  
- SwiftUI 6 years ago.  

If you wait for Apple to deliver on its promises, 
**you're going to miss out on the most important technological shift in a generation**.

The future is here today.
You don't have to wait.
With Ollama, you can start building the next generation of AI-powered apps 
_right now_.
